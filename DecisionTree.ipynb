{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvji58Rk8qcn8fNUFH3X3G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hrushikeshsahu19/ML_algorithm_custom_code/blob/main/DecisionTree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMfq1Z31PJ3z",
        "outputId": "07818d29-8a90-44f1-aac6-c2cbe55079b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.85\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth  # Maximum depth of the tree\n",
        "        self.tree = None  # The decision tree structure\n",
        "\n",
        "    def _gini_impurity(self, y):\n",
        "        \"\"\"\n",
        "        Compute the Gini Impurity for a given set of labels.\n",
        "\n",
        "        Parameters:\n",
        "        y (numpy.ndarray): Array of labels.\n",
        "\n",
        "        Returns:\n",
        "        float: Gini impurity value.\n",
        "        \"\"\"\n",
        "        _, counts = np.unique(y, return_counts=True)\n",
        "        prob_sq = (counts / len(y)) ** 2\n",
        "        return 1 - np.sum(prob_sq)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Find the best feature and value to split the data on.\n",
        "\n",
        "        Parameters:\n",
        "        X (numpy.ndarray): Feature matrix.\n",
        "        y (numpy.ndarray): Target vector.\n",
        "\n",
        "        Returns:\n",
        "        tuple: Best feature index and value for the split.\n",
        "        \"\"\"\n",
        "        best_gini = float('inf')\n",
        "        best_split = None\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        for feature_idx in range(n_features):\n",
        "            feature_values = np.unique(X[:, feature_idx])\n",
        "            for value in feature_values:\n",
        "                # Split the data based on the feature and value\n",
        "                left_mask = X[:, feature_idx] <= value\n",
        "                right_mask = ~left_mask\n",
        "\n",
        "                left_y, right_y = y[left_mask], y[right_mask]\n",
        "\n",
        "                # Calculate Gini impurity for the split\n",
        "                gini = (len(left_y) / n_samples) * self._gini_impurity(left_y) + \\\n",
        "                       (len(right_y) / n_samples) * self._gini_impurity(right_y)\n",
        "\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    best_split = (feature_idx, value)\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        Recursively build the decision tree.\n",
        "\n",
        "        Parameters:\n",
        "        X (numpy.ndarray): Feature matrix.\n",
        "        y (numpy.ndarray): Target vector.\n",
        "        depth (int): Current depth of the tree.\n",
        "\n",
        "        Returns:\n",
        "        dict: The decision tree structure.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        unique_classes = np.unique(y)\n",
        "\n",
        "        # If all samples have the same class or max depth is reached, return the class label\n",
        "        if len(unique_classes) == 1 or (self.max_depth and depth >= self.max_depth):\n",
        "            return unique_classes[0]\n",
        "\n",
        "        # Find the best split\n",
        "        feature_idx, value = self._best_split(X, y)\n",
        "\n",
        "        if feature_idx is None:\n",
        "            return np.random.choice(unique_classes)\n",
        "\n",
        "        # Split the data\n",
        "        left_mask = X[:, feature_idx] <= value\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        left_tree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
        "        right_tree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
        "\n",
        "        return {\n",
        "            'feature_idx': feature_idx,\n",
        "            'value': value,\n",
        "            'left': left_tree,\n",
        "            'right': right_tree\n",
        "        }\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the decision tree model.\n",
        "\n",
        "        Parameters:\n",
        "        X (numpy.ndarray): Feature matrix.\n",
        "        y (numpy.ndarray): Target vector.\n",
        "        \"\"\"\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _predict_single(self, x, tree):\n",
        "        \"\"\"\n",
        "        Predict the class label for a single sample based on the tree.\n",
        "\n",
        "        Parameters:\n",
        "        x (numpy.ndarray): Single sample.\n",
        "        tree (dict): The decision tree structure.\n",
        "\n",
        "        Returns:\n",
        "        int: Predicted class label.\n",
        "        \"\"\"\n",
        "        if isinstance(tree, dict):\n",
        "            feature_value = x[tree['feature_idx']]\n",
        "            if feature_value <= tree['value']:\n",
        "                return self._predict_single(x, tree['left'])\n",
        "            else:\n",
        "                return self._predict_single(x, tree['right'])\n",
        "        else:\n",
        "            return tree\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for the input data.\n",
        "\n",
        "        Parameters:\n",
        "        X (numpy.ndarray): Feature matrix.\n",
        "\n",
        "        Returns:\n",
        "        numpy.ndarray: Predicted class labels.\n",
        "        \"\"\"\n",
        "        return np.array([self._predict_single(x, self.tree) for x in X])\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    from sklearn.datasets import make_classification\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    # Generate a synthetic dataset\n",
        "    X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "    # Split into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train the decision tree model\n",
        "    model = DecisionTree(max_depth=10)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxJ+zabtYlyBeCYc3TBhnA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hrushikeshsahu19/ML_algorithm_custom_code/blob/main/NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onxvkGgzNZqB",
        "outputId": "36dc8b08-3255-45af-8b59-9a217cc4075a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.81\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.classes = None\n",
        "        self.mean = None\n",
        "        self.variance = None\n",
        "        self.priors = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the Naive Bayes model to the training data.\n",
        "\n",
        "        Parameters:\n",
        "        X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
        "        y (numpy.ndarray): Target vector of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self.classes = np.unique(y)  # Get unique class labels\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        # Initialize mean, variance, and priors\n",
        "        self.mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self.variance = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self.priors = np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self.classes):\n",
        "            X_c = X[y == c]  # Filter samples belonging to class c\n",
        "            self.mean[idx, :] = X_c.mean(axis=0)\n",
        "            self.variance[idx, :] = X_c.var(axis=0)\n",
        "            self.priors[idx] = X_c.shape[0] / n_samples  # P(c)\n",
        "\n",
        "    def _gaussian_probability(self, class_idx, x):\n",
        "        \"\"\"\n",
        "        Compute the Gaussian probability for a feature vector.\n",
        "\n",
        "        Parameters:\n",
        "        class_idx (int): Index of the class.\n",
        "        x (numpy.ndarray): Feature vector of shape (n_features,).\n",
        "\n",
        "        Returns:\n",
        "        numpy.ndarray: Probabilities of each feature for the class.\n",
        "        \"\"\"\n",
        "        mean = self.mean[class_idx]\n",
        "        variance = self.variance[class_idx]\n",
        "        numerator = np.exp(-((x - mean) ** 2) / (2 * variance))\n",
        "        denominator = np.sqrt(2 * np.pi * variance)\n",
        "        return numerator / denominator\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for the input data.\n",
        "\n",
        "        Parameters:\n",
        "        X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "        numpy.ndarray: Predicted class labels of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes)\n",
        "        posteriors = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        for idx, c in enumerate(self.classes):\n",
        "            prior = np.log(self.priors[idx])  # Log of prior probability\n",
        "            class_conditional = np.sum(np.log(self._gaussian_probability(idx, X)), axis=1)\n",
        "            posteriors[:, idx] = prior + class_conditional\n",
        "\n",
        "        return self.classes[np.argmax(posteriors, axis=1)]\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    from sklearn.datasets import make_classification\n",
        "\n",
        "    # Generate a synthetic dataset\n",
        "    X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "    # Split into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train the Naive Bayes model\n",
        "    model = NaiveBayes()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n"
      ]
    }
  ]
}